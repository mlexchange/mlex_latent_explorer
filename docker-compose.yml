services:
  prefect:
    image: prefecthq/prefect:2.14-python3.11
    command: prefect server start
    container_name: prefect-server
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://${PREFECT_DB_USER}:${PREFECT_DB_PW}@prefect_db:5432/${PREFECT_DB_NAME} # Needed if using postgres and not sqlite
      # - PREFECT_UI_API_URL=https://localhost/api. needed if nginx is handling ssl termination
      - PREFECT_LOGGING_LEVEL=DEBUG
    ports:
      - "127.0.0.1:4200:4200"
    depends_on:
      - prefect_db
    networks:
      mle_net:


  prefect_db:
    image: postgres:14.5-alpine
    container_name: prefect-db
    environment:
      - POSTGRES_USER=${PREFECT_DB_USER}
      - POSTGRES_PASSWORD=${PREFECT_DB_PW}
      - POSTGRES_DB=${PREFECT_DB_NAME}
    volumes:
      - ./data/prefect_db:/var/lib/postgresql/data:rw
    restart: unless-stopped
    networks:
      mle_net:


  tiled:
    # see the file ./tiled/deploy/config.yml for detailed configuration of tiled
    image: ghcr.io/bluesky/tiled:v0.1.0a118
    container_name: tiled-server
    ports:
      - "127.0.0.1:8000:8000"
    environment:
      - TILED_DB_USER=${TILED_DB_USER}
      - TILED_DB_PW=${TILED_DB_PW}
      - TILED_DB_NAME=${TILED_DB_NAME}
      - TILED_DB_SERVER=${TILED_DB_SERVER}
      - TILED_SINGLE_USER_API_KEY=${TILED_SINGLE_USER_API_KEY}
    volumes:
      - ./tiled/deploy:/deploy
      - ${READ_DIR}:/tiled_storage
    depends_on:
      - tiled_db
    networks:
      mle_net:


  tiled_db:
    image: postgres:14.5-alpine
    container_name: tiled-db
    environment:
      - POSTGRES_USER=${TILED_DB_USER}
      - POSTGRES_PASSWORD=${TILED_DB_PW}
      - POSTGRES_DB=${TILED_DB_NAME}
    volumes:
      - ./data/tiled_db:/var/lib/postgresql/data:rw
      - ${READ_DIR}:/tiled_storage
    restart: unless-stopped
    networks:
      mle_net:

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.22.0 
    container_name: mlflow-server
    command: >
      /bin/sh -c "pip install --no-cache-dir psycopg2-binary 'mlflow[auth]' &&
      mlflow server
      --backend-store-uri postgresql://${MLFLOW_DB_USER}:${MLFLOW_DB_PW}@mlflow_db:5432/${MLFLOW_DB_NAME}
      --host 0.0.0.0
      --port 5000
      --app-name basic-auth"
    ports:
      - "127.0.0.1:5000:5000"
    environment:
      - POSTGRES_USER=${MLFLOW_DB_USER}
      - POSTGRES_PASSWORD=${MLFLOW_DB_PW}
      - POSTGRES_DB=${MLFLOW_DB_NAME}
      - MLFLOW_TRACKING_USERNAME=${MLFLOW_TRACKING_USERNAME}
      - MLFLOW_TRACKING_PASSWORD=${MLFLOW_TRACKING_PASSWORD}
      - MLFLOW_FLASK_SERVER_SECRET_KEY=${MLFLOW_FLASK_SERVER_SECRET_KEY}
      - MLFLOW_AUTH_CONFIG_PATH=/basic_auth.ini
    depends_on:
      - mlflow_db
    networks:
      - mle_net
    volumes:
      - ./data/mlflow_storage:/mlartifacts:rw  # Persist MLflow models, logs, and artifacts
      - ./data/mlflow_auth:/mlflow_auth:rw # Dedicated volume for auth database
      - ./basic_auth.ini:/basic_auth.ini

  mlflow_db:
    image: postgres:14.5-alpine  # Lightweight PostgreSQL version
    container_name: mlflow-db
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${MLFLOW_DB_USER}
      - POSTGRES_PASSWORD=${MLFLOW_DB_PW}
      - POSTGRES_DB=${MLFLOW_DB_NAME}
    volumes:
      - ./data/mlflow_db:/var/lib/postgresql/data:rw
    networks:
      - mle_net

  latent-space-explorer:
    restart: "unless-stopped"
    container_name: "latent-space-explorer"
    build:
      context: "."
    command: "python frontend.py"
    environment:
      # Dash
      APP_HOST: "0.0.0.0"
      APP_PORT: "8070"
      # Data directories
      READ_DIR_MOUNT: "${READ_DIR}"   # Used to mount the read directory in podman jobs
      WRITE_DIR_MOUNT: "${WRITE_DIR}" # Used to mount the write directory in podman jobs
      READ_DIR: "/tiled_storage"
      WRITE_DIR: "/tiled_storage/writable"
      # Tiled
      DEFAULT_TILED_URI: '${DEFAULT_TILED_URI}'
      DATA_TILED_KEY: '${DATA_TILED_KEY}'
      RESULTS_TILED_URI: '${RESULTS_TILED_URI}'
      RESULTS_TILED_API_KEY: '${RESULTS_TILED_API_KEY}'
      # Prefect
      PREFECT_API_URL: '${PREFECT_API_URL}'
      FLOW_NAME: '${FLOW_NAME}'
      TIMEZONE: "${TIMEZONE}"
      PREFECT_TAGS: "${PREFECT_TAGS}"
      FLOW_TYPE: "${FLOW_TYPE}"
      CONTAINER_NETWORK: "${CONTAINER_NETWORK}"
      # Slurm jobs
      PARTITIONS_CPU: "${PARTITIONS_CPU}"
      MAX_TIME_CPU: "${MAX_TIME_CPU}"
      RESERVATIONS_CPU: "${RESERVATIONS_CPU}"
      PARTITIONS_GPU: "${PARTITIONS_GPU}"
      RESERVATIONS_GPU: "${RESERVATIONS_GPU}"
      MAX_TIME_GPU: "${MAX_TIME_GPU}"
      SUBMISSION_SSH_KEY: "${SUBMISSION_SSH_KEY}"
      FORWARD_PORTS: "${FORWARD_PORTS}"
      # Mode
      MODE: "development"
      USER: ${USER}
      # Live mode
      WEBSOCKET_URL: ${WEBSOCKET_URL:-ws://localhost:8765/lse}
      # MLflow
      MLFLOW_TRACKING_URI: '${MLFLOW_TRACKING_URI}'
      MLFLOW_TRACKING_USERNAME: '${MLFLOW_TRACKING_USERNAME}'
      MLFLOW_TRACKING_PASSWORD: '${MLFLOW_TRACKING_PASSWORD}'
      # Redis
      REDIS_HOST: '${REDIS_HOST}'
      REDIS_PORT: '${REDIS_PORT}'
    volumes:
      - ${READ_DIR}:/tiled_storage
      - ./src:/app/work/src
    ports:
      - "127.0.0.1:8070:8070"
    depends_on:
      - mlflow
      - tiled
      - prefect
    networks:
      mle_net:

  kvrocks:  
    image: docker.io/apache/kvrocks:latest 
    container_name: kvrocks  # Added container name 
    volumes:  
      - ./conf/kvrocks/kvrocks.conf:/etc/kvrocks/kvrocks.conf  
      - ./persist/kvrocks/data:/data  
    ports:
      - "127.0.0.1:6666:6666"  # Added port mapping
    networks:  
      mle_net: 

  # arroyo, arroyo_vec_sim and arroyo_sim are made optional via profiles
  # To run vector simulator, use
  # docker compose --profile vec_sim up
  # To run real image simulator, use
  # docker compose --profile arroyo --profile sim up

  arroyo:
    build:
      context: .
      dockerfile: Dockerfile_arroyo
    container_name: arroyo
    profiles:
      - arroyo
    volumes:
      - .:/app:Z
      - ./data/mlflow_cache:/mlflow_cache
    ports:
      - 127.0.0.1:8765:8765
    environment:
      NUMBA_DISABLE_JIT: ${NUMBA_DISABLE_JIT:-0}  # Default to enabled (0)
      NUMBA_CPU_NAME: ${NUMBA_CPU_NAME:-generic}
      NUMBA_CPU_FEATURES: ${NUMBA_CPU_FEATURES:-+neon}
      MLFLOW_CACHE_DIR: "/mlflow_cache"
      PYTHONUNBUFFERED: 1
      MALLOC_TRIM_THRESHOLD_: 0
    depends_on:
      - latent-space-explorer
    networks:
      mle_net:

  #Simulator 1: f_vec simulator service
  arroyo_vec_sim:
    command: python -m simulator.websocket_simulator
    build:
      context: .
      dockerfile: Dockerfile_arroyo
    container_name: arroyo_vec_sim
    profiles:
      - vec_sim
    volumes:
      - .:/app:Z
    ports:
      - 127.0.0.1:8765:8765
    networks:
      mle_net:

  # Simulator 2: real image simulator service
  sim_realistic:
    image: ghcr.io/als-computing/arroyosas:main
    container_name: sim_realistic
    command: python -m arroyosas.app.real_image_sim_cli
    profiles:
      - sim
    volumes:
      - ./data/test_sim_data:/data/test_data:ro # Mount test data repository containing TIFF images
    environment:
      - TILED_LIVE_API_KEY=${TILED_SINGLE_USER_API_KEY}
      - DATA_TILED_KEY=${DATA_TILED_KEY}
      - DYNACONF_TILED_POLLER__PUBLISH_ADDRESS=tcp://0.0.0.0:5000 # Make sure ZMQ publisher address is correctly set
    networks:
      mle_net:

  # Simulator 3: replay the runs saved in local db during smi beamtime
  sim_replay:
    image: ghcr.io/als-computing/arroyosas:main
    container_name: sim_replay
    command: python -m arroyosas.app.db_replay_sim_cli
    profiles:
      - sim_replay
    volumes:
      - ./data/vector_db_smi:/data/vector_db_smi:ro # Mount directory containing the vector database
    environment:
      - DB_PATH=/data/vector_db_smi/latent_vectors.db
      - TILED_API_KEY=${TILED_SINGLE_USER_API_KEY}
      - TILED_LIVE_API_KEY=${TILED_SINGLE_USER_API_KEY}
      - DYNACONF_TILED_POLLER__PUBLISH_ADDRESS=tcp://0.0.0.0:5000
    networks:
      mle_net:

networks:
  mle_net:
    name: mle_net
    driver: bridge
