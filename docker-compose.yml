services:
  prefect:
    image: prefecthq/prefect:2.14-python3.11
    command: prefect server start
    container_name: prefect-server
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://${PREFECT_DB_USER}:${PREFECT_DB_PW}@prefect_db:5432/${PREFECT_DB_NAME} # Needed if using postgres and not sqlite
      # - PREFECT_UI_API_URL=https://localhost/api. needed if nginx is handling ssl termination
      - PREFECT_LOGGING_LEVEL=DEBUG
    ports:
      - "127.0.0.1:4200:4200"
    depends_on:
      - prefect_db
    networks:
      mle_net:


  prefect_db:
    image: postgres:14.5-alpine
    container_name: prefect-db
    environment:
      - POSTGRES_USER=${PREFECT_DB_USER}
      - POSTGRES_PASSWORD=${PREFECT_DB_PW}
      - POSTGRES_DB=${PREFECT_DB_NAME}
    volumes:
      - ./data/prefect_db:/var/lib/postgresql/data:rw
    restart: unless-stopped
    networks:
      mle_net:


  tiled:
    # see the file ./tiled/deploy/config.yml for detailed configuration of tiled
    image: ghcr.io/bluesky/tiled:v0.1.0a118
    container_name: tiled-server
    ports:
      - "127.0.0.1:8000:8000"
    environment:
      - TILED_DB_USER=${TILED_DB_USER}
      - TILED_DB_PW=${TILED_DB_PW}
      - TILED_DB_NAME=${TILED_DB_NAME}
      - TILED_DB_SERVER=${TILED_DB_SERVER}
      - TILED_SINGLE_USER_API_KEY=${TILED_SINGLE_USER_API_KEY}
      # tiled server-side cache
      - TILED_RESOURCE_CACHE_MAX_SIZE=2048
      - TILED_RESOURCE_CACHE_TTU=300
    volumes:
      - ./tiled/deploy:/deploy
      - ${READ_DIR}:/tiled_storage
    depends_on:
      - tiled_db
    networks:
      mle_net:


  tiled_db:
    image: postgres:14.5-alpine
    container_name: tiled-db
    environment:
      - POSTGRES_USER=${TILED_DB_USER}
      - POSTGRES_PASSWORD=${TILED_DB_PW}
      - POSTGRES_DB=${TILED_DB_NAME}
    volumes:
      - ./data/tiled_db:/var/lib/postgresql/data:rw
      - ${READ_DIR}:/tiled_storage
    restart: unless-stopped
    networks:
      mle_net:

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.22.0 
    container_name: mlflow-server
    command: >
      /bin/sh -c "pip install --no-cache-dir psycopg2-binary 'mlflow[auth]' &&
      mlflow server
      --backend-store-uri postgresql://${MLFLOW_DB_USER}:${MLFLOW_DB_PW}@mlflow_db:5432/${MLFLOW_DB_NAME}
      --host 0.0.0.0
      --port 5000
      --app-name basic-auth"
    ports:
      - "127.0.0.1:5000:5000"
    environment:
      - POSTGRES_USER=${MLFLOW_DB_USER}
      - POSTGRES_PASSWORD=${MLFLOW_DB_PW}
      - POSTGRES_DB=${MLFLOW_DB_NAME}
      - MLFLOW_TRACKING_USERNAME=${MLFLOW_TRACKING_USERNAME}
      - MLFLOW_TRACKING_PASSWORD=${MLFLOW_TRACKING_PASSWORD}
      - MLFLOW_FLASK_SERVER_SECRET_KEY=${MLFLOW_FLASK_SERVER_SECRET_KEY}
      - MLFLOW_AUTH_CONFIG_PATH=/basic_auth.ini
    depends_on:
      - mlflow_db
    networks:
      - mle_net
    volumes:
      - ./data/mlflow_storage:/mlartifacts:rw  # Persist MLflow models, logs, and artifacts
      - ./data/mlflow_auth:/mlflow_auth:rw # Dedicated volume for auth database
      - ./basic_auth.ini:/basic_auth.ini

  mlflow_db:
    image: postgres:14.5-alpine  # Lightweight PostgreSQL version
    container_name: mlflow-db
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${MLFLOW_DB_USER}
      - POSTGRES_PASSWORD=${MLFLOW_DB_PW}
      - POSTGRES_DB=${MLFLOW_DB_NAME}
    volumes:
      - ./data/mlflow_db:/var/lib/postgresql/data:rw
    networks:
      - mle_net

  latent-space-explorer:
    restart: "unless-stopped"
    container_name: "latent-space-explorer"
    build:
      context: "."
    command: "python frontend.py"
    environment:
      # Dash
      APP_HOST: "0.0.0.0"
      APP_PORT: "8070"
      # Data directories
      READ_DIR_MOUNT: "${READ_DIR}"   # Used to mount the read directory in podman jobs
      WRITE_DIR_MOUNT: "${WRITE_DIR}" # Used to mount the write directory in podman jobs
      READ_DIR: "/tiled_storage"
      WRITE_DIR: "/tiled_storage/writable"
      # Tiled
      DEFAULT_TILED_URI: '${DEFAULT_TILED_URI}'
      DATA_TILED_KEY: '${DATA_TILED_KEY}'
      LIVE_TILED_API_KEY: '${LIVE_TILED_API_KEY}'
      RESULTS_TILED_URI: '${RESULTS_TILED_URI}'
      RESULTS_TILED_API_KEY: '${RESULTS_TILED_API_KEY}'
      TILED_REPLAY_PREFIX: 'beamlines/bl931/processed'  # NEW: Add this line
      # Prefect
      PREFECT_API_URL: '${PREFECT_API_URL}'
      FLOW_NAME: '${FLOW_NAME}'
      TIMEZONE: "${TIMEZONE}"
      PREFECT_TAGS: "${PREFECT_TAGS}"
      FLOW_TYPE: "${FLOW_TYPE}"
      CONTAINER_NETWORK: "${CONTAINER_NETWORK}"
      # Slurm jobs
      PARTITIONS_CPU: "${PARTITIONS_CPU}"
      MAX_TIME_CPU: "${MAX_TIME_CPU}"
      RESERVATIONS_CPU: "${RESERVATIONS_CPU}"
      PARTITIONS_GPU: "${PARTITIONS_GPU}"
      RESERVATIONS_GPU: "${RESERVATIONS_GPU}"
      MAX_TIME_GPU: "${MAX_TIME_GPU}"
      SUBMISSION_SSH_KEY: "${SUBMISSION_SSH_KEY}"
      FORWARD_PORTS: "${FORWARD_PORTS}"
      # Mode
      MODE: "development"
      USER: ${USER}
      # Live mode
      WEBSOCKET_URL: ${WEBSOCKET_URL:-ws://localhost:8765/lse_operator}
      # MLflow
      MLFLOW_TRACKING_URI: '${MLFLOW_TRACKING_URI}'
      MLFLOW_TRACKING_USERNAME: '${MLFLOW_TRACKING_USERNAME}'
      MLFLOW_TRACKING_PASSWORD: '${MLFLOW_TRACKING_PASSWORD}'
      # Redis
      REDIS_HOST: '${REDIS_HOST}'
      REDIS_PORT: '${REDIS_PORT}'
    volumes:
      - ${READ_DIR}:/tiled_storage
      - ./src:/app/work/src
      - ./data/tiled_cache:/tiled_cache
    ports:
      - "127.0.0.1:8070:8070"
    depends_on:
      - mlflow
      - tiled
      - prefect
    networks:
      mle_net:

  kvrocks:  
    image: docker.io/apache/kvrocks:latest 
    container_name: kvrocks  # Added container name 
    volumes:  
      - ./conf/kvrocks/kvrocks.conf:/etc/kvrocks/kvrocks.conf  
      - ./persist/kvrocks/data:/data  
    ports:
      - "127.0.0.1:6666:6666"  # Added port mapping
    networks:  
      mle_net: 

  # arroyo, arroyo_vec_sim and arroyo_sim are made optional via profiles
  # To run vector simulator, use
  # docker compose --profile vec_sim up
  # To run real image simulator, use
  # docker compose --profile arroyo --profile sim up

  # Original SAXS arroyo service
  arroyo:
    build:
      context: .
      dockerfile: Dockerfile_arroyo
    container_name: arroyo
    profiles:
      - arroyo
    volumes:
      - .:/app:Z
      - ./data/mlflow_cache:/mlflow_cache
      - ./data/vector_save:/data/vector_save
    ports:
      - 127.0.0.1:8765:8765
    environment:
      NUMBA_DISABLE_JIT: ${NUMBA_DISABLE_JIT:-0}  # Default to enabled (0)
      NUMBA_CPU_NAME: ${NUMBA_CPU_NAME:-generic}
      NUMBA_CPU_FEATURES: ${NUMBA_CPU_FEATURES:-+neon}
      MLFLOW_CACHE_DIR: "/mlflow_cache"
      PYTHONUNBUFFERED: 1
      MALLOC_TRIM_THRESHOLD_: 0
      # MLflow
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_TRACKING_USERNAME: ${MLFLOW_TRACKING_USERNAME}
      MLFLOW_TRACKING_PASSWORD: ${MLFLOW_TRACKING_PASSWORD}
      MLFLOW_TRACKING_INSECURE_TLS: ${MLFLOW_TRACKING_INSECURE_TLS:-False}
      # Redis
      REDIS_HOST: ${REDIS_HOST}
      REDIS_PORT: ${REDIS_PORT}
      # Tiled Publisher
      RESULTS_TILED_URI: '${RESULTS_TILED_URI}'
      RESULTS_TILED_API_KEY: '${RESULTS_TILED_API_KEY}'
      # User info
      USER: ${USER}
    depends_on:
      - latent-space-explorer
    networks:
      mle_net:

  # NEW: XPS arroyo service (XPS_WEBSOCKET_URL environment variable REMOVED)
  arroyo_xps:
    build:
      context: .
      dockerfile: Dockerfile_arroyo_xps
    container_name: arroyo_xps
    profiles:
      - arroyo_xps
    volumes:
      - .:/app:Z
      - ./data/mlflow_cache:/mlflow_cache
      - ./data/vector_save:/data/vector_save
    ports:
      - 127.0.0.1:8765:8765  
    environment:
      NUMBA_DISABLE_JIT: ${NUMBA_DISABLE_JIT:-0}
      NUMBA_CPU_NAME: ${NUMBA_CPU_NAME:-generic}
      NUMBA_CPU_FEATURES: ${NUMBA_CPU_FEATURES:-+neon}
      MLFLOW_CACHE_DIR: "/mlflow_cache"
      PYTHONUNBUFFERED: 1
      MALLOC_TRIM_THRESHOLD_: 0
      # MLflow
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI}
      MLFLOW_TRACKING_USERNAME: ${MLFLOW_TRACKING_USERNAME}
      MLFLOW_TRACKING_PASSWORD: ${MLFLOW_TRACKING_PASSWORD}
      MLFLOW_TRACKING_INSECURE_TLS: ${MLFLOW_TRACKING_INSECURE_TLS:-False}
      # Redis
      REDIS_HOST: ${REDIS_HOST}
      REDIS_PORT: ${REDIS_PORT}
      # Tiled Publisher
      RESULTS_TILED_URI: '${RESULTS_TILED_URI}'
      RESULTS_TILED_API_KEY: '${RESULTS_TILED_API_KEY}'
      # User info
      USER: ${USER}
      # XPS_WEBSOCKET_URL removed - now configured in settings.yaml
    depends_on:
      - latent-space-explorer
    networks:
      mle_net:

  #Simulator 1: f_vec simulator service
  arroyo_vec_sim:
    command: python -m simulator.websocket_simulator
    build:
      context: .
      dockerfile: Dockerfile_arroyo
    container_name: arroyo_vec_sim
    profiles:
      - vec_sim
    volumes:
      - .:/app:Z
    ports:
      - 127.0.0.1:8765:8765
    networks:
      mle_net:

  # Simulator 2: Unified real image simulator
  sim_unified:
    image: ghcr.io/als-computing/arroyosas:main
    container_name: sim_unified
    command: >
      python -m arroyosas.app.unified_sim_cli
      --mode ${SIM_MODE:-direct}
    profiles:
      - sim
    volumes:
      - ./data:/data:rw
    environment:
      # SIM_MODE: use direct, db_replay or local_tiled
      - SIM_MODE=${SIM_MODE:-direct} 
      # API key
      - TILED_LIVE_API_KEY=${LIVE_TILED_API_KEY} #TILED_LIVE_API_KEY is required by DynaconfFormat
      # db_replay mode
      - DB_PATH=${DB_PATH:-/data/vector_db_replay/latent_vectors.db}
      - TILED_ENV=${TILED_ENV:-dev}
      # local_tiled mode
      - URL_FILE=${URL_FILE:-/data/tiled_url/Nafion_p25_30_run_1.json}
      # ZMQ
      - DYNACONF_TILED_POLLER__PUBLISH_ADDRESS=tcp://0.0.0.0:5000
      - DYNACONF_TILED_POLLER__ZMQ_FRAME_PUBLISHER__ADDRESS=tcp://0.0.0.0:5000
    networks:
      mle_net:

  # Image Ingestion Service - run this first to prepare data for Simulator 4
  ingest_local_images:
    image: ghcr.io/als-computing/arroyosas:main
    container_name: ingest_local_images
    command: python -m arroyosas.app.ingest_local_images
    profiles:
      - ingest_images
    volumes:
      - ./data/733_april24_2025_png/Nafion_p25_30_run_1:/data/Nafion_p25_30_run_1:ro  # Mount directory containing the images
      - ./data:/data:rw  # Mount directory for the URL file
    environment:
      - DEFAULT_IMAGE_FOLDER=/data/Nafion_p25_30_run_1
      - TILED_URI=${DEFAULT_TILED_URI}
      - TILED_API_KEY=${TILED_SINGLE_USER_API_KEY}
      - TILED_CONTAINER=Nafion_p25_30_run_1
      - URL_FILE=/data/tiled_url/Nafion_p25_30_run_1.json
    networks:
      mle_net:

  # XPS WebSocket Simulator - for testing XPS data processing
  sim_xps:
    image: ghcr.io/als-computing/arroyosas:main
    container_name: sim_xps
    command: >
      python -m arroyosas.app.xps_websocket_simulator
      --host 0.0.0.0
      --port 8001
      --data-file /data/shot_averages/combined_all_bin_averages.npy
      --cycles 1
      --pause 1
    profiles:
      - sim_xps
    volumes:
      - ./data/shot_averages:/data/shot_averages:ro
    ports:
      - 127.0.0.1:8001:8001
    environment:
      - PYTHONUNBUFFERED=1
      - WEBSOCKET_PING_TIMEOUT=60  # Add this (default is ~20s)
      - WEBSOCKET_PING_INTERVAL=20  # Add this (default is ~5s)
    networks:
      mle_net:

networks:
  mle_net:
    name: mle_net
    driver: bridge